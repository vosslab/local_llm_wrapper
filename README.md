# local_llm_wrapper
Local LLM wrapper with a simple, stable text-in text-out API. Supports multiple local backends (for example Ollama and Apple) via pluggable transports, with structured output helpers and robust parsing plus retry for unattended runs.
